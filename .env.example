# ============================================================================
# n8n Co-Pilot Configuration
# ============================================================================
# Copy this file to .env and configure your settings

# ----------------------------------------------------------------------------
# n8n Connection (REQUIRED)
# ----------------------------------------------------------------------------
# URL of your n8n instance
N8N_API_URL=http://localhost:5678

# n8n API Key - Get this from n8n Settings > API
# REQUIRED for most features (workflow management, validation, etc.)
N8N_API_KEY=your-n8n-api-key-here

# ----------------------------------------------------------------------------
# n8n Session Authentication (OPTIONAL - for full node catalog)
# ----------------------------------------------------------------------------
# These credentials enable access to n8n internal endpoints like /types/nodes.json
# which provides the complete node catalog with full schema information.
# Without these, the server falls back to extracting nodes from existing workflows.
#
# NOTE: MFA-enabled accounts are NOT supported
N8N_USERNAME=your-n8n-email@example.com
N8N_PASSWORD=your-n8n-password

# ----------------------------------------------------------------------------
# LLM Backend Settings
# ----------------------------------------------------------------------------
# Backend selection (priority order): docker-model-runner, ollama, vllm, auto
# 'auto' will try Docker Model Runner first, then fall back to Ollama
LLM_BACKEND=auto

# ----------------------------------------------------------------------------
# Docker Model Runner (RECOMMENDED - Docker Desktop 4.54+)
# ----------------------------------------------------------------------------
# Docker Model Runner provides high-performance vLLM inference on NVIDIA GPUs
# It's built into Docker Desktop 4.54+ with native vLLM support.
#
# Leave empty for auto-detection (recommended)
# Inside container: http://model-runner.docker.internal
# From host: http://localhost:12434
DOCKER_MODEL_RUNNER_URL=

# Default model for Docker Model Runner
# vLLM models (high-performance): ai/smollm2-vllm, ai/llama3.2-vllm
# GGUF models (llama.cpp): ai/smollm2, ai/llama3.2
DOCKER_MODEL_RUNNER_MODEL=ai/smollm2-vllm

# GPU memory utilization (0.0-1.0, default: 0.9)
DOCKER_MODEL_RUNNER_GPU_MEMORY=0.9

# ----------------------------------------------------------------------------
# Ollama Settings (Fallback)
# ----------------------------------------------------------------------------
# Ollama server URL (cross-platform fallback)
OLLAMA_URL=http://localhost:11434

# Override auto-detected models (optional)
# Leave empty to use hardware-based auto-detection
# EMBEDDING_MODEL=nomic-embed-text
# GENERATION_MODEL=llama3.2:3b

# ----------------------------------------------------------------------------
# Standalone vLLM (Custom Deployments)
# ----------------------------------------------------------------------------
# For custom vLLM deployments on Linux servers
# VLLM_EMBEDDING_URL=http://localhost:8001
# VLLM_GENERATION_URL=http://localhost:8002

# ----------------------------------------------------------------------------
# HTTP Server Settings (for HTTP mode)
# ----------------------------------------------------------------------------
# Port for HTTP server (used with MCP_MODE=http)
PORT=3001

# Authentication token for HTTP API
# Generate with: openssl rand -base64 32
AUTH_TOKEN=

# Server mode: 'stdio' for Claude Desktop, 'http' for Open WebUI/API
MCP_MODE=stdio

# ----------------------------------------------------------------------------
# Validation Settings
# ----------------------------------------------------------------------------
# Enable dry-run validation (creates test workflow in n8n, then deletes)
ENABLE_DRY_RUN=true

# Strict mode - fail on warnings too (default: false)
STRICT_MODE=false

# ----------------------------------------------------------------------------
# Node Restrictions
# ----------------------------------------------------------------------------
# Allow community nodes (default: false - only official n8n nodes allowed)
ALLOW_COMMUNITY_NODES=false

# Comma-separated list of allowed community node types (if ALLOW_COMMUNITY_NODES=false)
# COMMUNITY_NODE_WHITELIST=n8n-nodes-browserless,n8n-nodes-chatgpt

# ----------------------------------------------------------------------------
# Open WebUI Integration
# ----------------------------------------------------------------------------
# Secret key for Open WebUI sessions
# Generate with: openssl rand -base64 32
WEBUI_SECRET_KEY=change-me-in-production

# ----------------------------------------------------------------------------
# Logging
# ----------------------------------------------------------------------------
# Log level: debug, info, warn, error
LOG_LEVEL=info

# ----------------------------------------------------------------------------
# Timezone (for Docker)
# ----------------------------------------------------------------------------
TIMEZONE=America/Detroit
