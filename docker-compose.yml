version: "3.9"

# ============================================================================
# n8n Co-Pilot Stack
# 
# Complete setup with:
# - n8n: Workflow automation platform
# - MCP: Co-Pilot server (live sync, validation, LLM)
# - Ollama: Dual-model LLM backend (embedding + generation)
# - Open WebUI: Human interface for chat-based workflow building
#
# Quick Start:
#   1. Copy .env.example to .env and configure
#   2. docker compose up -d
#   3. Access n8n at http://localhost:5678
#   4. Access Open WebUI at http://localhost:3000
#
# The MCP server syncs nodes LIVE from n8n - no manual rebuild needed!
# ============================================================================

services:
  # n8n - Workflow Automation Platform
  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    restart: unless-stopped
    environment:
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - NODE_ENV=production
      - GENERIC_TIMEZONE=${TIMEZONE:-America/Detroit}
    ports:
      - "5678:5678"
    volumes:
      - n8n-data:/home/node/.n8n
    networks:
      - copilot-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:5678/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # MCP Server - n8n Co-Pilot
  # Runs in HTTP mode to serve both Open WebUI and direct API access
  mcp:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: n8n-mcp
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - MCP_MODE=http
      - PORT=3001
      - N8N_API_URL=http://n8n:5678
      - N8N_API_KEY=${N8N_API_KEY:-}
      - OLLAMA_URL=http://ollama:11434
      - AUTH_TOKEN=${AUTH_TOKEN:-}
      - ENABLE_DRY_RUN=${ENABLE_DRY_RUN:-true}
      - LOG_LEVEL=${LOG_LEVEL:-info}
    ports:
      - "3001:3001"
    networks:
      - copilot-network
    depends_on:
      n8n:
        condition: service_healthy
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Ollama - Dual-Model LLM Backend
  # Hosts both embedding model (nomic-embed-text) and generation model (auto-selected)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - copilot-network
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  # Open WebUI - Human Interface
  # Connect to the MCP server for chat-based workflow building
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    restart: unless-stopped
    environment:
      - WEBUI_NAME=n8n Co-Pilot
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-change-me-in-production}
      - OLLAMA_BASE_URL=http://ollama:11434
      # Enable function calling for MCP integration
      - ENABLE_OPENAI_API=true
      - OPENAI_API_BASE_URL=http://mcp:3001/openwebui
      - OPENAI_API_KEY=not-needed
    ports:
      - "3000:8080"
    volumes:
      - open-webui-data:/app/backend/data
    networks:
      - copilot-network
    depends_on:
      - mcp
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/v1/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

volumes:
  n8n-data:
    driver: local
  ollama-data:
    driver: local
  open-webui-data:
    driver: local

networks:
  copilot-network:
    driver: bridge

# ============================================================================
# USAGE NOTES
# ============================================================================
#
# 1. FIRST-TIME SETUP:
#    - Create .env file from .env.example
#    - Set N8N_API_KEY (get from n8n Settings > API)
#    - Set AUTH_TOKEN for MCP HTTP security
#    - Set WEBUI_SECRET_KEY for Open WebUI security
#
# 2. ACCESSING SERVICES:
#    - n8n:       http://localhost:5678
#    - Open WebUI: http://localhost:3000
#    - MCP API:   http://localhost:3001
#
# 3. HOW IT WORKS:
#    - MCP server connects to n8n via API
#    - Node catalog syncs LIVE from n8n (every 5 min)
#    - All workflow changes validated before reaching n8n
#    - LLM provides semantic validation and chat assistance
#
# 4. LLM MODELS:
#    The system auto-detects your hardware and selects optimal models:
#    - Embedding: nomic-embed-text (for semantic search)
#    - Generation: llama3.2:1b/3b or gemma (for chat/validation)
#    Models are pulled automatically on first run.
#
# 5. CLAUDE DESKTOP INTEGRATION:
#    For MCP stdio mode (not HTTP), run separately:
#    docker run -it --rm \
#      -e N8N_API_URL=http://host.docker.internal:5678 \
#      -e N8N_API_KEY=your-key \
#      n8n-mcp:latest
#
# 6. GPU ACCELERATION:
#    Uncomment the deploy section under ollama service
#    to enable NVIDIA GPU acceleration for faster LLM inference.
#
