# docker-compose.yml
# Dual Nano LLM Architecture with vLLM Services
# For optimized builds with BuildKit, use: docker-compose -f docker-compose.buildkit.yml up
version: '3.8'

services:
  # ============================================
  # vLLM Embedding Service
  # EmbeddingGemma 300M (or Nomic-Embed-Text v1.5)
  # Handles semantic understanding and knowledge graph queries
  # ============================================
  vllm-embedding:
    image: vllm/vllm-openai:${VLLM_EMBEDDING_VERSION:-latest}
    container_name: vllm-embedding
    restart: unless-stopped

    environment:
      MODEL_ID: ${EMBEDDING_MODEL:-sentence-transformers/sentence-similarity-base-multilingual}
      CUDA_VISIBLE_DEVICES: ${EMBEDDING_GPU_ID:-0}
      SERVED_MODEL_NAME: embedding-model
      API_KEY: ${VLLM_API_KEY:-}

    volumes:
      - vllm-embedding-cache:/root/.cache

    ports:
      - "${EMBEDDING_SERVER_PORT:-8001}:8000"

    # Resource allocation for embedding model (lightweight)
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    networks:
      - n8n-mcp-network

  # ============================================
  # vLLM Generation Service
  # Nemotron Nano 4B, Llama 3.2 3B, or Llama 3.2 1B
  # Handles text generation and workflow creation
  # ============================================
  vllm-generation:
    image: vllm/vllm-openai:${VLLM_GENERATION_VERSION:-latest}
    container_name: vllm-generation
    restart: unless-stopped

    environment:
      MODEL_ID: ${GENERATION_MODEL:-meta-llama/Llama-2-7b-hf}
      CUDA_VISIBLE_DEVICES: ${GENERATION_GPU_ID:-0}
      SERVED_MODEL_NAME: generation-model
      API_KEY: ${VLLM_API_KEY:-}
      DTYPE: ${GENERATION_DTYPE:-auto}  # auto, float16, float32, bfloat16
      MAX_TOKENS: ${GENERATION_MAX_TOKENS:-2048}
      ENFORCE_EAGER: ${GENERATION_ENFORCE_EAGER:-false}

    volumes:
      - vllm-generation-cache:/root/.cache

    ports:
      - "${GENERATION_SERVER_PORT:-8002}:8000"

    # Resource allocation for generation model (larger)
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G

    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

    networks:
      - n8n-mcp-network

  # ============================================
  # Main n8n MCP Server
  # Orchestrates dual nano LLM models
  # ============================================
  n8n-mcp:
    build:
      context: .
      dockerfile: Dockerfile
    image: n8n-mcp:unified-v1
    container_name: n8n-mcp-unified
    restart: unless-stopped
    depends_on:
      vllm-embedding:
        condition: service_healthy
      vllm-generation:
        condition: service_healthy

    # Environment configuration
    environment:
      # Mode configuration (consolidated, http, or stdio)
      MCP_MODE: ${MCP_MODE:-consolidated}
      USE_FIXED_HTTP: ${USE_FIXED_HTTP:-true}  # Use fixed implementation for stability
      AUTH_TOKEN: ${AUTH_TOKEN:-}

      # Auto-update configuration (for simple-auto mode)
      # GITHUB_TOKEN: ${GITHUB_TOKEN:-}  # Disabled: no n8n packages in runtime image
      UPDATE_INTERVAL_MINUTES: ${UPDATE_INTERVAL_MINUTES:-15}

      # Application settings
      NODE_ENV: ${NODE_ENV:-production}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      PORT: ${PORT:-3000}

      # Database
      NODE_DB_PATH: ${NODE_DB_PATH:-/app/data/nodes.db}
      REBUILD_ON_START: ${REBUILD_ON_START:-false}

      # Optional: n8n API configuration (enables 16 additional management tools)
      # Uncomment and configure to enable n8n workflow management
      N8N_API_URL: ${N8N_API_URL}
      N8N_API_KEY: ${N8N_API_KEY}
      N8N_API_TIMEOUT: ${N8N_API_TIMEOUT:-30000}
      N8N_API_MAX_RETRIES: ${N8N_API_MAX_RETRIES:-3}

      # N8N Browser Authentication (for visual verification)
      N8N_USERNAME: ${N8N_USERNAME:-chrisboyd1993@gmail.com}
      N8N_PASSWORD: ${N8N_PASSWORD:-AlastorB2024!@#}

      # ============================================
      # Dual Nano LLM Configuration
      # ============================================
      # Embedding Model (for semantic understanding)
      EMBEDDING_MODEL: ${EMBEDDING_MODEL:-sentence-transformers/sentence-similarity-base-multilingual}
      EMBEDDING_BASE_URL: ${EMBEDDING_BASE_URL:-http://vllm-embedding:8000}
      EMBEDDING_SERVER_PORT: ${EMBEDDING_SERVER_PORT:-8001}

      # Generation Model (for text generation)
      GENERATION_MODEL: ${GENERATION_MODEL:-meta-llama/Llama-2-7b-hf}
      GENERATION_BASE_URL: ${GENERATION_BASE_URL:-http://vllm-generation:8000}
      GENERATION_SERVER_PORT: ${GENERATION_SERVER_PORT:-8002}

      # vLLM Configuration
      VLLM_API_KEY: ${VLLM_API_KEY:-}
      VLLM_EMBEDDING_VERSION: ${VLLM_EMBEDDING_VERSION:-latest}
      VLLM_GENERATION_VERSION: ${VLLM_GENERATION_VERSION:-latest}
    
    # Volumes for persistence and Docker access
    volumes:
      - n8n-mcp-data:/app/data
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker socket for container access
    
    # Port mapping
    ports:
      - "${PORT:-3000}:3000"
    
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

    # Network for service communication
    networks:
      - n8n-mcp-network

# ============================================
# Named volumes for data persistence and caching
# ============================================
volumes:
  # Main n8n MCP data volume
  n8n-mcp-data:
    driver: local

  # vLLM embedding model cache (lighter weight)
  vllm-embedding-cache:
    driver: local

  # vLLM generation model cache (heavier weight)
  vllm-generation-cache:
    driver: local

# ============================================
# Docker network for internal service communication
# ============================================
networks:
  n8n-mcp-network:
    driver: bridge
