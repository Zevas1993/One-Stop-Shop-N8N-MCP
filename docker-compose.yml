version: "3.9"

services:
  # Official n8n Instance - Workflow Automation Platform
  n8n:
    image: n8nio/n8n:latest
    container_name: n8n-instance
    restart: unless-stopped
    environment:
      - N8N_HOST=n8n
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - NODE_ENV=production
      # Optional: Set admin user
      # - N8N_DEFAULT_USER_PASSWORD=your-secure-password
      # Optional: Enable n8n analytics
      # - N8N_DIAGNOSTICS_ENABLED=false
    ports:
      - "5678:5678"
    volumes:
      # n8n data persistence
      - n8n-data:/home/node/.n8n
      # Share n8n node_modules with MCP for runtime detection
      - n8n-modules:/shared/n8n-modules:ro
    networks:
      - nano-network
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:5678/healthz",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # MCP Server - n8n Node Documentation & GraphRAG Learning System
  mcp:
    build:
      context: .
      dockerfile: Dockerfile
      # Build args can be passed here if needed
    container_name: mcp-server
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - MCP_MODE=stdio
      - AUTH_TOKEN=${AUTH_TOKEN:-}
      - LOG_LEVEL=info
      # n8n API integration (optional - for workflow management features)
      - N8N_API_URL=http://n8n:5678/api
      - N8N_API_KEY=${N8N_API_KEY:-}
    volumes:
      # MCP data persistence (nodes.db, GraphRAG knowledge graph)
      - mcp-data:/app/data
      # Mount n8n's node_modules as read-only for runtime version detection
      - n8n-modules:/shared/n8n-modules:ro
    networks:
      - nano-network
    depends_on:
      n8n:
        condition: service_healthy
    stdin_open: true
    tty: true
    # Note: MCP runs in stdio mode, not exposed on network

  # Ollama - Nano LLM Inference Server (Embedding + Generation)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    ports:
      - "11434:11434"
    volumes:
      # Ollama model cache and data
      - ollama-data:/root/.ollama
    networks:
      - nano-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  # Open WebUI - Natural Language Interface for Orchestration
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    restart: unless-stopped
    environment:
      - WEBUI_NAME=n8n Learning System
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-your-secret-key-change-me}
      - WEBUI_URL=http://open-webui:3000
      # Optional: Configure default model
      # - OLLAMA_BASE_URL=http://ollama:11434
    ports:
      - "3000:8080"
    volumes:
      # Open WebUI data persistence (conversations, user settings, models)
      - open-webui-data:/app/backend/data
    networks:
      - nano-network
    depends_on:
      - mcp
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/v1/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

volumes:
  # n8n Workflows & Settings
  n8n-data:
    driver: local

  # MCP Database & GraphRAG Knowledge Graph
  mcp-data:
    driver: local

  # n8n node_modules (shared with MCP for runtime detection)
  # This is where MCP will detect n8n version changes
  n8n-modules:
    driver: local

  # Open WebUI Conversations & User Data
  open-webui-data:
    driver: local

  # Ollama Model Cache
  # Stores downloaded Ollama models to avoid re-downloading
  ollama-data:
    driver: local

networks:
  nano-network:
    driver: bridge
# Architecture Notes:
# ====================
#
# 1. SERVICE ISOLATION:
#    - n8n: Workflow automation platform (official image)
#    - mcp: Node documentation + GraphRAG learning system (custom image)
#    - open-webui: Natural language interface for orchestration
#
# 2. SHARED VOLUME STRATEGY:
#    The "n8n-modules" volume is the critical link between services:
#    - n8n writes its node_modules to this volume at startup
#    - MCP mounts it as read-only to detect n8n version
#    - MCP's entrypoint script reads /shared/n8n-modules/n8n/package.json
#    - If version changed, MCP auto-triggers npm run rebuild
#    - This ensures nodes.db is always in sync with n8n version
#
# 3. AUTOMATIC VERSION DETECTION:
#    When MCP container starts:
#    a) Waits up to 60 seconds for n8n to initialize
#    b) Reads n8n version from shared volume
#    c) Compares to last known version in /app/data/.n8n-versions
#    d) If different, rebuilds nodes.db by scanning n8n packages
#    e) Stores new version for next startup
#
# 4. DATA PERSISTENCE:
#    Each service has dedicated volume for its data:
#    - n8n-data: Workflows, credentials, executions
#    - mcp-data: nodes.db, GraphRAG graph storage
#    - open-webui-data: Conversations, user preferences
#    All volumes persist across container restarts
#
# 5. INDEPENDENT UPDATES:
#    Users can update services independently:
#    - `docker compose pull && docker compose up -d`
#    - n8n update: MCP detects version change and rebuilds
#    - MCP update: New image, same nodes.db unless n8n also updated
#    - No manual intervention required
#
# 6. NANO LLM INTEGRATION (Ollama):
#    The system uses Ollama for Nano LLM inference:
#    - Ollama service provides model hosting on port 11434
#    - MCP connects to Ollama for embedding and generation tasks
#    - GraphRAG Backend (Python-based knowledge graph)
#    These work together to learn from user workflows and improve suggestions
#
# QUICK START:
# ============
# 1. Create .env file with authentication tokens:
#    echo "AUTH_TOKEN=$(openssl rand -base64 32)" > .env
#    echo "N8N_API_KEY=$(openssl rand -base64 32)" >> .env
#    echo "WEBUI_SECRET_KEY=$(openssl rand -base64 32)" >> .env
#
# 2. Start all services:
#    docker compose up -d
#
# 3. Check status:
#    docker compose ps
#    docker compose logs -f mcp  # Watch MCP version detection
#
# 4. Access services:
#    - n8n: http://localhost:5678
#    - Open WebUI: http://localhost:3000
#    - MCP: Stdio mode (used by Claude Desktop)
#
# 5. Stop all services:
#    docker compose down
#
# 6. Clean up volumes (⚠️  WARNING: deletes all data):
#    docker compose down -v
#
