# Local Nano LLM Architecture - Offline-First MCP Server

**Date:** October 31, 2025
**Status:** ğŸ”„ IN PROGRESS - Planning Phase
**Objective:** Enable direct user interaction with MCP server via hardware-aware local nano LLMs, without requiring external AI agents

---

## Overview

Transform the MCP server from **agent-only tool** to **user-facing application** that:

1. **Runs locally** via Docker Desktop (ultra-simple setup)
2. **Selects nano LLM based on hardware** (Grok's recommendations for CPU/RAM/GPU)
3. **Accepts n8n API key directly** from end user
4. **Discusses workflow ideas** with end user in natural conversation
5. **Generates workflows autonomously** using nano agent orchestrator
6. **Validates and deploys** to n8n instance
7. **Works fully offline** (no external AI service required)

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Docker Desktop (User)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            Web UI (Browser: localhost:3000)                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  1. Setup Page                                       â”‚  â”‚ â”‚
â”‚  â”‚  â”‚     - n8n API URL input                              â”‚  â”‚ â”‚
â”‚  â”‚  â”‚     - n8n API Key input (secure storage)            â”‚  â”‚ â”‚
â”‚  â”‚  â”‚     - Hardware detection results                     â”‚  â”‚ â”‚
â”‚  â”‚  â”‚     - Selected nano LLM info                         â”‚  â”‚ â”‚
â”‚  â”‚  â”‚                                                      â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  2. Conversation Page                               â”‚  â”‚ â”‚
â”‚  â”‚  â”‚     - Chat interface with nano LLM                  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚     - Discuss workflow ideas                        â”‚  â”‚ â”‚
â”‚  â”‚  â”‚     - View generated workflows                      â”‚  â”‚ â”‚
â”‚  â”‚  â”‚     - Deploy workflows to n8n                       â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â†‘â†“ HTTP API                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         MCP Server (Node.js + Local Nano LLM)             â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ Hardware Detection Module                            â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - CPU cores, RAM, GPU detection                     â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - Grok recommendations mapping                       â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - Selected LLM: Phi-3.5 / Mixtral / etc            â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ Local Nano LLM Orchestrator                          â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - Manages LLM lifecycle                             â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - System prompt with n8n expertise                  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - Context window management                         â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - Memory of workflow discussion                     â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ Nano Agent Orchestrator (Existing)                  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - PatternAgent â†’ GraphRAG â†’ WorkflowAgent â†’ Validatorâ”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - Autonomous workflow generation                    â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ Existing Tools (27 doc + 16 management)             â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - Node search & documentation                       â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - Workflow creation/update/validation               â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ - Execution management                              â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         SQLite Database (nodes.db)                         â”‚ â”‚
â”‚  â”‚         Pre-built with all n8n node info                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  n8n Instance    â”‚
                    â”‚  (Remote or      â”‚
                    â”‚   Localhost:5678)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Components to Implement

### 1. Hardware Detection Module

**Purpose:** Detect user's hardware and recommend appropriate nano LLM

**Implementation Location:** `src/ai/hardware-detector.ts`

```typescript
export interface HardwareProfile {
  cpuCores: number;
  ramGbytes: number;
  hasGpu: boolean;
  gpuVram?: number;
  osType: string;
  recommendedLlm: NanoLLMOption;
}

export enum NanoLLMOption {
  PHI_3_MINI = 'phi-3.5-mini',      // 3.8B - Minimal (2GB RAM, 2 cores)
  PHI_3_SMALL = 'phi-3.5-small',    // ~7B - Low resource (4GB RAM, 4 cores)
  MIXTRAL_7B = 'mixtral-7b',        // 7B - Balanced (8GB RAM, 4 cores)
  LLAMA_2_13B = 'llama-2-13b',      // 13B - Better quality (16GB RAM, 8 cores)
  NEURAL_CHAT_7B = 'neural-chat-7b' // 7B - Specialized for chat
}

export class HardwareDetector {
  detectHardware(): HardwareProfile
  getRecommendedLLM(profile: HardwareProfile): NanoLLMOption
  validateGpuAvailability(): boolean
}
```

**Grok-Provided LLM Recommendations** (from previous discussion):
- **â‰¤2GB RAM, 2 cores:** Phi-3.5-mini (3.8B parameters)
- **2-4GB RAM, 2-4 cores:** Phi-3.5-small (~7B)
- **4-8GB RAM, 4-8 cores:** Mixtral-7B or Neural-Chat-7B
- **8-16GB RAM, 8+ cores:** Llama-2-13B or larger
- **16GB+ RAM, with GPU:** Larger models with GPU acceleration

### 2. Local Nano LLM Orchestrator

**Purpose:** Manage nano LLM lifecycle and conversation state

**Implementation Location:** `src/ai/local-llm-orchestrator.ts`

```typescript
export interface ConversationContext {
  messages: Array<{ role: 'user' | 'assistant'; content: string }>;
  workflowIdeas: string[];
  generatedWorkflows: WorkflowGenerationResult[];
  n8nApiKey: string;
  n8nApiUrl: string;
}

export class LocalLLMOrchestrator {
  private llm: LLMClient;
  private context: ConversationContext;
  private systemPrompt: string;

  constructor(llmOption: NanoLLMOption, config: LocalLLMConfig) {
    // Initialize LLM (Ollama, LocalAI, or Hugging Face)
    // Load system prompt for n8n workflow expertise
  }

  async chat(userMessage: string): Promise<string>
  async generateWorkflow(idea: string): Promise<WorkflowGenerationResult>
  async validateAndDeployWorkflow(workflow: any): Promise<DeploymentResult>
  getConversationContext(): ConversationContext
  clearContext(): void
}
```

**System Prompt Content:**
```
You are an expert n8n workflow architect assistant.

Your capabilities:
1. Discuss workflow ideas with users in natural conversation
2. Recommend n8n nodes for specific tasks
3. Design multi-step workflows
4. Understand n8n concepts: nodes, connections, triggers, execution, expressions

When users describe workflow ideas:
- Ask clarifying questions about requirements
- Suggest specific nodes from available n8n catalog (525 nodes)
- Explain how nodes connect and pass data
- Help refine workflow requirements

You have access to:
- Complete n8n node documentation (525 nodes)
- Nano agent orchestrator for autonomous workflow generation
- n8n API for validation and deployment

Always strive to create practical, validated workflows ready for deployment.
```

### 3. HTTP API for Web UI

**Purpose:** Expose nano LLM conversation and setup functionality

**Implementation Location:** `src/http/local-llm-routes.ts`

```
POST /api/setup/detect-hardware
  Response: HardwareProfile + Recommended LLM

POST /api/setup/configure
  Input: { n8nApiUrl, n8nApiKey }
  Response: { status, message }

POST /api/chat
  Input: { message: string }
  Response: { response: string, suggestedActions: string[] }

POST /api/workflow/generate
  Input: { idea: string, constraints?: object }
  Response: { workflow, explanation, deployment }

POST /api/workflow/deploy
  Input: { workflowId }
  Response: { deploymentStatus, executionLink }

GET /api/context
  Response: ConversationContext
```

### 4. Web UI Components

**Purpose:** Simple, intuitive interface for users

**Implementation Location:** `src/web-ui/`

**Key Pages:**
1. **Setup Page** (`/setup`)
   - Hardware detection results
   - Selected nano LLM info
   - n8n API configuration form
   - Connection test

2. **Conversation Page** (`/chat`)
   - Chat interface with nano LLM
   - Workflow ideas history
   - Generated workflows viewer
   - Deploy button

3. **Workflow Viewer** (`/workflows`)
   - Generated workflows list
   - Visual workflow editor (optional)
   - Validation results
   - Deployment history

---

## Implementation Phases

### Phase 1: Hardware Detection & LLM Selection âœ… PLANNED
- [ ] Implement HardwareDetector
- [ ] Create LLM selection logic based on Grok's recommendations
- [ ] Add detection to HTTP API

### Phase 2: Local LLM Integration (Ollama/LocalAI) ğŸ”„ IN PROGRESS
- [ ] Integrate Ollama for local LLM hosting
- [ ] Create LocalLLMOrchestrator
- [ ] Write comprehensive system prompt
- [ ] Implement conversation context management

### Phase 3: HTTP API & Web UI ğŸ“‹ PLANNED
- [ ] Create HTTP routes for setup and chat
- [ ] Build simple web UI (HTML/CSS/JS)
- [ ] Integrate with existing MCP server

### Phase 4: Integration with Nano Agents ğŸ“‹ PLANNED
- [ ] Connect nano LLM to nano agent orchestrator
- [ ] Enable workflow generation from chat
- [ ] Implement validation and deployment flow

### Phase 5: Docker Desktop Setup ğŸ“‹ PLANNED
- [ ] Docker Compose configuration
- [ ] Environment setup guide
- [ ] One-click deployment script

---

## Data Flow: User Conversation â†’ Workflow â†’ Deployment

```
User: "I want to send Slack notifications when Airtable records are updated"
  â†“
LocalLLMOrchestrator.chat()
  â†“
Nano LLM: "Great idea! This involves:
           - Airtable trigger
           - Data transformation
           - Slack notification
           Let me generate a workflow..."
  â†“
Nano LLM calls LocalLLMOrchestrator.generateWorkflow()
  â†“
Nano Agent Orchestrator.executePipeline()
  - PatternAgent: "Notification Pattern (85% confidence)"
  - GraphRAG: "Airtable + Slack nodes compatible"
  - WorkflowAgent: "Generated workflow with 3 nodes"
  - ValidatorAgent: "Validation passed âœ…"
  â†“
Workflow JSON returned to Web UI
  â†“
User clicks "Deploy"
  â†“
n8nApiClient.createWorkflow() â†’ Deployed âœ…
  â†“
User receives confirmation: "Workflow deployed! ID: 123"
```

---

## Configuration Files

### Docker Compose Setup

```yaml
version: '3.8'
services:
  mcp-server:
    image: n8n-mcp:local-llm
    ports:
      - "3000:3000"        # Web UI
      - "5678:5678"        # n8n instance (optional)
    environment:
      NODE_ENV: production
      MCP_MODE: http
      AUTH_TOKEN: ${AUTH_TOKEN}
      ENABLE_LOCAL_LLM: true
      LLM_OPTION: auto # or specific: phi-3.5-mini, mixtral-7b, etc.
    volumes:
      - ./config:/app/config
      - ./data:/app/data
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
    environment:
      OLLAMA_HOST: 0.0.0.0:11434
```

### Environment Configuration

```env
# MCP Server
NODE_ENV=production
MCP_MODE=http
AUTH_TOKEN=your-secure-token
PORT=3000

# Local LLM
ENABLE_LOCAL_LLM=true
LLM_OPTION=auto              # or: phi-3.5-mini, mixtral-7b, etc.
OLLAMA_BASE_URL=http://ollama:11434

# n8n Configuration (set by user via Web UI)
N8N_API_URL=http://localhost:5678
N8N_API_KEY=xxxx
```

---

## Hardware-LLM Mapping (Grok's Recommendations)

| Hardware | LLM Option | Model Size | Requirements | Performance |
|----------|-----------|-----------|--------------|-------------|
| **Minimal** | Phi-3.5-mini | 3.8B | 2GB RAM, 2 cores | Fast, basic |
| **Low-End** | Phi-3.5-small | 7B | 4GB RAM, 4 cores | Good, capable |
| **Mid-Range** | Mixtral-7B | 7B-MOE | 8GB RAM, 4 cores | Excellent, fast |
| **Standard** | Llama-2-13B | 13B | 16GB RAM, 8 cores | Outstanding, slower |
| **With GPU** | GPU-accelerated model | Variable | GPU VRAM + RAM | Fastest, best quality |

---

## Next Steps

1. **Implement HardwareDetector** - Detect user's hardware specifications
2. **Integrate Ollama** - Local LLM hosting with auto-download
3. **Create LocalLLMOrchestrator** - Manage conversation and workflow generation
4. **Build HTTP API routes** - Expose setup and chat endpoints
5. **Create Web UI** - Simple HTML/CSS interface for users
6. **Docker setup** - One-click Docker Desktop deployment
7. **Testing** - E2E testing with various hardware profiles

---

## Success Criteria

âœ… User can start MCP server via Docker Desktop with one command
âœ… Hardware detection automatically selects appropriate nano LLM
âœ… User enters n8n API key via Web UI (no config files)
âœ… User can discuss workflow ideas with nano LLM in natural conversation
âœ… Nano LLM can trigger nano agent orchestrator to generate workflows
âœ… Generated workflows are validated and deployable to n8n
âœ… Entire flow works offline (no external AI service required)
âœ… System works on hardware from 2GB RAM/2 cores up to high-end machines

---

## Benefits

ğŸ¯ **For End Users:**
- Ultra-simple setup (just `docker compose up`)
- Direct interaction without Claude Desktop
- No external AI service dependency
- Works fully offline
- Conversational workflow design

ğŸ¯ **For Developers:**
- Reuses existing nano agent orchestrator
- Leverages existing n8n tool catalog
- Hardware-aware optimization
- Extensible architecture

ğŸ¯ **For n8n Community:**
- Makes n8n automation accessible to everyone
- Removes barriers to workflow creation
- Enables conversational workflow design
- Offline-first approach for privacy/security
